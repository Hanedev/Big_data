# 📊 Projet Final - Traitement de Données Massives avec Apache Spark

## 🔍 Objectif

Ce projet a été réalisé dans le cadre d’un cours de Big Data. Il consiste à exploiter la puissance d’**Apache Spark** pour analyser des données massives de manière distribuée. L’objectif principal est d’extraire des insights à partir de données réelles via **Spark SQL** et **PySpark**.

## 🛠️ Technologies utilisées

- **Apache Spark** (via PySpark)
- **Spark SQL**
- **Python 3**
- **Jupyter Notebook**

## 📁 Contenu

- `Projet Final Apache Spark.ipynb` : Notebook contenant tout le pipeline de traitement, de la lecture des données à l’analyse finale.
- Requêtes SQL sur Spark
- Nettoyage et transformation de données avec PySpark
- Création de DataFrames distribués
- Analyses statistiques et croisées
- Visualisations des résultats

## 📊 Aperçu des analyses

- Statistiques globales sur les jeux de données
- Requêtes spécifiques en SQL (tri, agrégation, jointures)

## 📦 Installation

### Prérequis

- Python 3.8+
- Apache Spark
- Jupyter Notebook
- PySpark

### Lancer le projet

```bash
git clone https://github.com/Hanedev/Big_data.git
cd Big_data
jupyter notebook
