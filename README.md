# ğŸ“Š Projet Final - Traitement de DonnÃ©es Massives avec Apache Spark

## ğŸ” Objectif

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre dâ€™un cours de Big Data. Il consiste Ã  exploiter la puissance dâ€™**Apache Spark** pour analyser des donnÃ©es massives de maniÃ¨re distribuÃ©e. Lâ€™objectif principal est dâ€™extraire des insights Ã  partir de donnÃ©es rÃ©elles via **Spark SQL** et **PySpark**.

## ğŸ› ï¸ Technologies utilisÃ©es

- **Apache Spark** (via PySpark)
- **Spark SQL**
- **Python 3**
- **Jupyter Notebook**

## ğŸ“ Contenu

- `Projet Final Apache Spark.ipynb` : Notebook contenant tout le pipeline de traitement, de la lecture des donnÃ©es Ã  lâ€™analyse finale.
- RequÃªtes SQL sur Spark
- Nettoyage et transformation de donnÃ©es avec PySpark
- CrÃ©ation de DataFrames distribuÃ©s
- Analyses statistiques et croisÃ©es
- Visualisations des rÃ©sultats

## ğŸ“Š AperÃ§u des analyses

- Statistiques globales sur les jeux de donnÃ©es
- RequÃªtes spÃ©cifiques en SQL (tri, agrÃ©gation, jointures)

## ğŸ“¦ Installation

### PrÃ©requis

- Python 3.8+
- Apache Spark
- Jupyter Notebook
- PySpark

### Lancer le projet

```bash
git clone https://github.com/Hanedev/Big_data.git
cd Big_data
jupyter notebook
